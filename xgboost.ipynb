{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(csv_file):\n",
    "    df = pd.read_csv(csv_file, sep=',')\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        rating = row['rating']\n",
    "        user_avg_rating = row['user_avg_rating']\n",
    "        recipe_avg_rating = row['recipe_avg_rating']\n",
    "\n",
    "        n_steps = row['n_steps']\n",
    "        n_ingredients = row['n_ingredients']\n",
    "        minutes = row['minutes']\n",
    "        n_recipe_words = row['n_recipe_words']\n",
    "\n",
    "        n_review_words = row['n_review_words']\n",
    "        n_positive = row['n_positive']\n",
    "        n_negative = row['n_negative']\n",
    "        n_exclamation = row['n_exclamation']\n",
    "\n",
    "        features.append([user_avg_rating, recipe_avg_rating,\n",
    "                       n_steps, n_ingredients, minutes, n_recipe_words,\n",
    "                       n_review_words, n_positive, n_negative, n_exclamation])\n",
    "        \n",
    "        labels.append(rating)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "X_train, y_train = get_data('train.csv')\n",
    "X_valid, y_valid = get_data('valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 10\n",
      "7023 10\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_train[0]))\n",
    "print(len(X_valid), len(X_valid[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into DMatrix format, which is optimized for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "# define the evaluation sets: training and validation sets\n",
    "evals = [(dtrain, 'train'), (dvalid, 'eval')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunable parameters of XGBoost\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # For regression tasks\n",
    "    'eval_metric': 'rmse',  # Use RMSE as evaluation metric\n",
    "    'eta': 0.1,  # Learning rate\n",
    "    'max_depth': 6,  # Maximum depth of trees\n",
    "    'subsample': 0.8,  # Subsampling ratio\n",
    "    'colsample_bytree': 0.8  # Column sampling ratio\n",
    "}\n",
    "\n",
    "num_round = 1000  # Max number of boosting rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.87734\teval-rmse:1.34280\n",
      "[1]\ttrain-rmse:0.80033\teval-rmse:1.33541\n",
      "[2]\ttrain-rmse:0.73204\teval-rmse:1.33053\n",
      "[3]\ttrain-rmse:0.67085\teval-rmse:1.32522\n",
      "[4]\ttrain-rmse:0.61652\teval-rmse:1.32488\n",
      "[5]\ttrain-rmse:0.58657\teval-rmse:1.32383\n",
      "[6]\ttrain-rmse:0.54643\teval-rmse:1.31190\n",
      "[7]\ttrain-rmse:0.50625\teval-rmse:1.31122\n",
      "[8]\ttrain-rmse:0.47025\teval-rmse:1.31109\n",
      "[9]\ttrain-rmse:0.44326\teval-rmse:1.30943\n",
      "[10]\ttrain-rmse:0.42001\teval-rmse:1.30443\n",
      "[11]\ttrain-rmse:0.40033\teval-rmse:1.30252\n",
      "[12]\ttrain-rmse:0.37819\teval-rmse:1.30410\n",
      "[13]\ttrain-rmse:0.35955\teval-rmse:1.30477\n",
      "[14]\ttrain-rmse:0.34297\teval-rmse:1.30653\n",
      "[15]\ttrain-rmse:0.32819\teval-rmse:1.30753\n",
      "[16]\ttrain-rmse:0.31649\teval-rmse:1.30922\n",
      "[17]\ttrain-rmse:0.30558\teval-rmse:1.31138\n",
      "[18]\ttrain-rmse:0.29612\teval-rmse:1.31315\n",
      "[19]\ttrain-rmse:0.28817\teval-rmse:1.31511\n",
      "[20]\ttrain-rmse:0.28301\teval-rmse:1.31450\n",
      "[21]\ttrain-rmse:0.27592\teval-rmse:1.31610\n",
      "[22]\ttrain-rmse:0.26935\teval-rmse:1.31735\n",
      "[23]\ttrain-rmse:0.26498\teval-rmse:1.31864\n",
      "[24]\ttrain-rmse:0.26197\teval-rmse:1.31673\n",
      "[25]\ttrain-rmse:0.25718\teval-rmse:1.31829\n",
      "[26]\ttrain-rmse:0.25294\teval-rmse:1.31996\n",
      "[27]\ttrain-rmse:0.25019\teval-rmse:1.32141\n",
      "[28]\ttrain-rmse:0.24738\teval-rmse:1.32246\n",
      "[29]\ttrain-rmse:0.24534\teval-rmse:1.32342\n",
      "[30]\ttrain-rmse:0.24289\teval-rmse:1.32443\n",
      "[31]\ttrain-rmse:0.24066\teval-rmse:1.32538\n",
      "[32]\ttrain-rmse:0.23930\teval-rmse:1.32696\n",
      "[33]\ttrain-rmse:0.23758\teval-rmse:1.32769\n",
      "[34]\ttrain-rmse:0.23589\teval-rmse:1.32837\n",
      "[35]\ttrain-rmse:0.23407\teval-rmse:1.33054\n",
      "[36]\ttrain-rmse:0.23295\teval-rmse:1.33083\n",
      "[37]\ttrain-rmse:0.23163\teval-rmse:1.33579\n",
      "[38]\ttrain-rmse:0.23075\teval-rmse:1.33511\n",
      "[39]\ttrain-rmse:0.22898\teval-rmse:1.33435\n",
      "[40]\ttrain-rmse:0.22795\teval-rmse:1.33483\n",
      "[41]\ttrain-rmse:0.22718\teval-rmse:1.33450\n",
      "[42]\ttrain-rmse:0.22651\teval-rmse:1.33508\n",
      "[43]\ttrain-rmse:0.22565\teval-rmse:1.33528\n",
      "[44]\ttrain-rmse:0.22528\teval-rmse:1.33542\n",
      "[45]\ttrain-rmse:0.22509\teval-rmse:1.33587\n",
      "[46]\ttrain-rmse:0.22477\teval-rmse:1.33633\n",
      "[47]\ttrain-rmse:0.22438\teval-rmse:1.33602\n",
      "[48]\ttrain-rmse:0.22364\teval-rmse:1.33749\n",
      "[49]\ttrain-rmse:0.22266\teval-rmse:1.33738\n",
      "[50]\ttrain-rmse:0.22181\teval-rmse:1.33868\n",
      "[51]\ttrain-rmse:0.22099\teval-rmse:1.33881\n",
      "[52]\ttrain-rmse:0.22012\teval-rmse:1.33900\n",
      "[53]\ttrain-rmse:0.21944\teval-rmse:1.33910\n",
      "[54]\ttrain-rmse:0.21878\teval-rmse:1.33898\n",
      "[55]\ttrain-rmse:0.21865\teval-rmse:1.33884\n",
      "[56]\ttrain-rmse:0.21793\teval-rmse:1.33879\n",
      "[57]\ttrain-rmse:0.21759\teval-rmse:1.33877\n",
      "[58]\ttrain-rmse:0.21690\teval-rmse:1.33901\n",
      "[59]\ttrain-rmse:0.21613\teval-rmse:1.33985\n",
      "[60]\ttrain-rmse:0.21557\teval-rmse:1.33994\n"
     ]
    }
   ],
   "source": [
    "# Train the model with early stopping\n",
    "start_time = time()\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_round,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,  # Stop after 50 rounds of no improvement\n",
    "    verbose_eval=True  # Print evaluation results during training\n",
    ")\n",
    "\n",
    "end_time = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best iteration: 11\n",
      "training time: 0.16177916526794434\n"
     ]
    }
   ],
   "source": [
    "# after training, get the best iteration\n",
    "best_iteration = bst.best_iteration\n",
    "print(f\"best iteration: {best_iteration}\")\n",
    "print(f\"training time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evalute on test set\n",
      "test_mse: 1.9453\n",
      "evaluation time: 0.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "print(\"evalute on test set\")\n",
    "X_test, y_test = get_data('test.csv')\n",
    "# convert to DMatrix format\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"test_mse: {test_mse:.4f}\")\n",
    "print(f\"evaluation time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.95580\teval-rmse:1.34616\n",
      "[1]\ttrain-rmse:0.94803\teval-rmse:1.34473\n",
      "[2]\ttrain-rmse:0.94037\teval-rmse:1.34337\n",
      "[3]\ttrain-rmse:0.93274\teval-rmse:1.34372\n",
      "[4]\ttrain-rmse:0.92526\teval-rmse:1.34400\n",
      "[5]\ttrain-rmse:0.92055\teval-rmse:1.34256\n",
      "[6]\ttrain-rmse:0.91351\teval-rmse:1.34227\n",
      "[7]\ttrain-rmse:0.90614\teval-rmse:1.34235\n",
      "[8]\ttrain-rmse:0.89888\teval-rmse:1.34247\n",
      "[9]\ttrain-rmse:0.89211\teval-rmse:1.34257\n",
      "[10]\ttrain-rmse:0.88542\teval-rmse:1.34287\n",
      "[11]\ttrain-rmse:0.87885\teval-rmse:1.34292\n",
      "[12]\ttrain-rmse:0.87191\teval-rmse:1.34296\n",
      "[13]\ttrain-rmse:0.86506\teval-rmse:1.34191\n",
      "[14]\ttrain-rmse:0.85826\teval-rmse:1.34197\n",
      "[15]\ttrain-rmse:0.85151\teval-rmse:1.34204\n",
      "[16]\ttrain-rmse:0.84489\teval-rmse:1.34213\n",
      "[17]\ttrain-rmse:0.83829\teval-rmse:1.34223\n",
      "[18]\ttrain-rmse:0.83179\teval-rmse:1.34121\n",
      "[19]\ttrain-rmse:0.82536\teval-rmse:1.34154\n",
      "[20]\ttrain-rmse:0.81945\teval-rmse:1.34164\n",
      "[21]\ttrain-rmse:0.81315\teval-rmse:1.34047\n",
      "[22]\ttrain-rmse:0.80698\teval-rmse:1.34080\n",
      "[23]\ttrain-rmse:0.80081\teval-rmse:1.34085\n",
      "[24]\ttrain-rmse:0.79514\teval-rmse:1.34092\n",
      "[25]\ttrain-rmse:0.78913\teval-rmse:1.34101\n",
      "[26]\ttrain-rmse:0.78319\teval-rmse:1.34111\n",
      "[27]\ttrain-rmse:0.77733\teval-rmse:1.34143\n",
      "[28]\ttrain-rmse:0.77153\teval-rmse:1.34171\n",
      "[29]\ttrain-rmse:0.76582\teval-rmse:1.34177\n",
      "[30]\ttrain-rmse:0.76015\teval-rmse:1.34184\n",
      "[31]\ttrain-rmse:0.75455\teval-rmse:1.34183\n",
      "[32]\ttrain-rmse:0.74899\teval-rmse:1.34099\n",
      "[33]\ttrain-rmse:0.74356\teval-rmse:1.34107\n",
      "[34]\ttrain-rmse:0.73816\teval-rmse:1.34120\n",
      "[35]\ttrain-rmse:0.73281\teval-rmse:1.34038\n",
      "[36]\ttrain-rmse:0.72916\teval-rmse:1.34006\n",
      "[37]\ttrain-rmse:0.72391\teval-rmse:1.34009\n",
      "[38]\ttrain-rmse:0.71913\teval-rmse:1.34038\n",
      "[39]\ttrain-rmse:0.71398\teval-rmse:1.34044\n",
      "[40]\ttrain-rmse:0.70891\teval-rmse:1.34053\n",
      "[41]\ttrain-rmse:0.70428\teval-rmse:1.34079\n",
      "[42]\ttrain-rmse:0.70085\teval-rmse:1.34051\n",
      "[43]\ttrain-rmse:0.69745\teval-rmse:1.34013\n",
      "[44]\ttrain-rmse:0.69408\teval-rmse:1.33979\n",
      "[45]\ttrain-rmse:0.68926\teval-rmse:1.33978\n",
      "[46]\ttrain-rmse:0.68447\teval-rmse:1.33978\n",
      "[47]\ttrain-rmse:0.68010\teval-rmse:1.33987\n",
      "[48]\ttrain-rmse:0.67544\teval-rmse:1.34037\n",
      "[49]\ttrain-rmse:0.67119\teval-rmse:1.34044\n",
      "[50]\ttrain-rmse:0.66700\teval-rmse:1.34045\n",
      "[51]\ttrain-rmse:0.66283\teval-rmse:1.34034\n",
      "[52]\ttrain-rmse:0.65835\teval-rmse:1.34062\n",
      "[53]\ttrain-rmse:0.65523\teval-rmse:1.34029\n",
      "[54]\ttrain-rmse:0.65082\teval-rmse:1.34052\n",
      "[55]\ttrain-rmse:0.64687\teval-rmse:1.34040\n",
      "[56]\ttrain-rmse:0.64256\teval-rmse:1.34067\n",
      "[57]\ttrain-rmse:0.63866\teval-rmse:1.34072\n",
      "[58]\ttrain-rmse:0.63445\teval-rmse:1.34006\n",
      "[59]\ttrain-rmse:0.63031\teval-rmse:1.34037\n",
      "[60]\ttrain-rmse:0.62625\teval-rmse:1.34037\n",
      "[61]\ttrain-rmse:0.62210\teval-rmse:1.33977\n",
      "[62]\ttrain-rmse:0.62167\teval-rmse:1.33909\n",
      "[63]\ttrain-rmse:0.61883\teval-rmse:1.33886\n",
      "[64]\ttrain-rmse:0.61481\teval-rmse:1.33913\n",
      "[65]\ttrain-rmse:0.61129\teval-rmse:1.33919\n",
      "[66]\ttrain-rmse:0.60858\teval-rmse:1.33904\n",
      "[67]\ttrain-rmse:0.60461\teval-rmse:1.33847\n",
      "[68]\ttrain-rmse:0.60072\teval-rmse:1.33873\n",
      "[69]\ttrain-rmse:0.59810\teval-rmse:1.33862\n",
      "[70]\ttrain-rmse:0.59433\teval-rmse:1.33806\n",
      "[71]\ttrain-rmse:0.59103\teval-rmse:1.33804\n",
      "[72]\ttrain-rmse:0.58773\teval-rmse:1.33812\n",
      "[73]\ttrain-rmse:0.58416\teval-rmse:1.33840\n",
      "[74]\ttrain-rmse:0.58061\teval-rmse:1.33869\n",
      "[75]\ttrain-rmse:0.57709\teval-rmse:1.33888\n",
      "[76]\ttrain-rmse:0.57398\teval-rmse:1.33878\n",
      "[77]\ttrain-rmse:0.57051\teval-rmse:1.33827\n",
      "[78]\ttrain-rmse:0.56700\teval-rmse:1.33778\n",
      "[79]\ttrain-rmse:0.56459\teval-rmse:1.33765\n",
      "[80]\ttrain-rmse:0.56159\teval-rmse:1.33755\n",
      "[81]\ttrain-rmse:0.55816\teval-rmse:1.33707\n",
      "[82]\ttrain-rmse:0.55536\teval-rmse:1.33696\n",
      "[83]\ttrain-rmse:0.55305\teval-rmse:1.33688\n",
      "[84]\ttrain-rmse:0.55269\teval-rmse:1.33631\n",
      "[85]\ttrain-rmse:0.54955\teval-rmse:1.33658\n",
      "[86]\ttrain-rmse:0.54643\teval-rmse:1.33685\n",
      "[87]\ttrain-rmse:0.54332\teval-rmse:1.33703\n",
      "[88]\ttrain-rmse:0.54013\teval-rmse:1.33659\n",
      "[89]\ttrain-rmse:0.53745\teval-rmse:1.33645\n",
      "[90]\ttrain-rmse:0.53440\teval-rmse:1.33671\n",
      "[91]\ttrain-rmse:0.53228\teval-rmse:1.33665\n",
      "[92]\ttrain-rmse:0.52925\teval-rmse:1.33693\n",
      "[93]\ttrain-rmse:0.52716\teval-rmse:1.33666\n",
      "[94]\ttrain-rmse:0.52409\teval-rmse:1.33625\n",
      "[95]\ttrain-rmse:0.52205\teval-rmse:1.33615\n",
      "[96]\ttrain-rmse:0.51917\teval-rmse:1.33643\n",
      "[97]\ttrain-rmse:0.51626\teval-rmse:1.33604\n",
      "[98]\ttrain-rmse:0.51351\teval-rmse:1.33618\n",
      "[99]\ttrain-rmse:0.51158\teval-rmse:1.33618\n",
      "[100]\ttrain-rmse:0.50967\teval-rmse:1.33612\n",
      "[101]\ttrain-rmse:0.50736\teval-rmse:1.33600\n",
      "[102]\ttrain-rmse:0.50505\teval-rmse:1.33594\n",
      "[103]\ttrain-rmse:0.50279\teval-rmse:1.33581\n",
      "[104]\ttrain-rmse:0.50009\teval-rmse:1.33531\n",
      "[105]\ttrain-rmse:0.49747\teval-rmse:1.33557\n",
      "[106]\ttrain-rmse:0.49530\teval-rmse:1.33558\n",
      "[107]\ttrain-rmse:0.49274\teval-rmse:1.33586\n",
      "[108]\ttrain-rmse:0.49246\teval-rmse:1.33536\n",
      "[109]\ttrain-rmse:0.49001\teval-rmse:1.33490\n",
      "[110]\ttrain-rmse:0.48757\teval-rmse:1.33456\n",
      "[111]\ttrain-rmse:0.48510\teval-rmse:1.33426\n",
      "[112]\ttrain-rmse:0.48273\teval-rmse:1.33452\n",
      "[113]\ttrain-rmse:0.48026\teval-rmse:1.33423\n",
      "[114]\ttrain-rmse:0.47830\teval-rmse:1.33412\n",
      "[115]\ttrain-rmse:0.47583\teval-rmse:1.33386\n",
      "[116]\ttrain-rmse:0.47347\teval-rmse:1.33347\n",
      "[117]\ttrain-rmse:0.47117\teval-rmse:1.33315\n",
      "[118]\ttrain-rmse:0.46930\teval-rmse:1.33316\n",
      "[119]\ttrain-rmse:0.46774\teval-rmse:1.33318\n",
      "[120]\ttrain-rmse:0.46550\teval-rmse:1.33343\n",
      "[121]\ttrain-rmse:0.46394\teval-rmse:1.33347\n",
      "[122]\ttrain-rmse:0.46178\teval-rmse:1.33374\n",
      "[123]\ttrain-rmse:0.45960\teval-rmse:1.33399\n",
      "[124]\ttrain-rmse:0.45750\teval-rmse:1.33426\n",
      "[125]\ttrain-rmse:0.45538\teval-rmse:1.33398\n",
      "[126]\ttrain-rmse:0.45322\teval-rmse:1.33375\n",
      "[127]\ttrain-rmse:0.45118\teval-rmse:1.33401\n",
      "[128]\ttrain-rmse:0.44909\teval-rmse:1.33376\n",
      "[129]\ttrain-rmse:0.44750\teval-rmse:1.33324\n",
      "[130]\ttrain-rmse:0.44558\teval-rmse:1.33349\n",
      "[131]\ttrain-rmse:0.44363\teval-rmse:1.33328\n",
      "[132]\ttrain-rmse:0.44172\teval-rmse:1.33303\n",
      "[133]\ttrain-rmse:0.43982\teval-rmse:1.33283\n",
      "[134]\ttrain-rmse:0.43848\teval-rmse:1.33286\n",
      "[135]\ttrain-rmse:0.43720\teval-rmse:1.33293\n",
      "[136]\ttrain-rmse:0.43542\teval-rmse:1.33318\n",
      "[137]\ttrain-rmse:0.43417\teval-rmse:1.33320\n",
      "[138]\ttrain-rmse:0.43237\teval-rmse:1.33302\n",
      "[139]\ttrain-rmse:0.43112\teval-rmse:1.33300\n",
      "[140]\ttrain-rmse:0.42941\teval-rmse:1.33278\n",
      "[141]\ttrain-rmse:0.42763\teval-rmse:1.33300\n",
      "[142]\ttrain-rmse:0.42592\teval-rmse:1.33282\n",
      "[143]\ttrain-rmse:0.42473\teval-rmse:1.33284\n",
      "[144]\ttrain-rmse:0.42308\teval-rmse:1.33263\n",
      "[145]\ttrain-rmse:0.42179\teval-rmse:1.33256\n",
      "[146]\ttrain-rmse:0.42065\teval-rmse:1.33256\n",
      "[147]\ttrain-rmse:0.41902\teval-rmse:1.33240\n",
      "[148]\ttrain-rmse:0.41742\teval-rmse:1.33225\n",
      "[149]\ttrain-rmse:0.41584\teval-rmse:1.33209\n",
      "[150]\ttrain-rmse:0.41432\teval-rmse:1.33233\n",
      "[151]\ttrain-rmse:0.41315\teval-rmse:1.33227\n",
      "[152]\ttrain-rmse:0.41162\teval-rmse:1.33214\n",
      "[153]\ttrain-rmse:0.41010\teval-rmse:1.33201\n",
      "[154]\ttrain-rmse:0.40855\teval-rmse:1.33189\n",
      "[155]\ttrain-rmse:0.40710\teval-rmse:1.33211\n",
      "[156]\ttrain-rmse:0.40568\teval-rmse:1.33191\n",
      "[157]\ttrain-rmse:0.40419\teval-rmse:1.33213\n",
      "[158]\ttrain-rmse:0.40279\teval-rmse:1.33202\n",
      "[159]\ttrain-rmse:0.40138\teval-rmse:1.33224\n",
      "[160]\ttrain-rmse:0.40000\teval-rmse:1.33210\n",
      "[161]\ttrain-rmse:0.39862\teval-rmse:1.33233\n",
      "[162]\ttrain-rmse:0.39761\teval-rmse:1.33192\n",
      "[163]\ttrain-rmse:0.39662\teval-rmse:1.33144\n",
      "[164]\ttrain-rmse:0.39538\teval-rmse:1.33125\n",
      "[165]\ttrain-rmse:0.39410\teval-rmse:1.33148\n",
      "[166]\ttrain-rmse:0.39321\teval-rmse:1.33154\n",
      "[167]\ttrain-rmse:0.39232\teval-rmse:1.33157\n",
      "[168]\ttrain-rmse:0.39105\teval-rmse:1.33180\n",
      "[169]\ttrain-rmse:0.38977\teval-rmse:1.33170\n",
      "[170]\ttrain-rmse:0.38885\teval-rmse:1.33132\n",
      "[171]\ttrain-rmse:0.38763\teval-rmse:1.33122\n",
      "[172]\ttrain-rmse:0.38675\teval-rmse:1.33086\n",
      "[173]\ttrain-rmse:0.38591\teval-rmse:1.33086\n",
      "[174]\ttrain-rmse:0.38509\teval-rmse:1.33089\n",
      "[175]\ttrain-rmse:0.38393\teval-rmse:1.33103\n",
      "[176]\ttrain-rmse:0.38277\teval-rmse:1.33120\n",
      "[177]\ttrain-rmse:0.38162\teval-rmse:1.33137\n",
      "[178]\ttrain-rmse:0.38048\teval-rmse:1.33154\n",
      "[179]\ttrain-rmse:0.38035\teval-rmse:1.33123\n",
      "[180]\ttrain-rmse:0.37956\teval-rmse:1.33089\n",
      "[181]\ttrain-rmse:0.37847\teval-rmse:1.33111\n",
      "[182]\ttrain-rmse:0.37771\teval-rmse:1.33113\n",
      "[183]\ttrain-rmse:0.37698\teval-rmse:1.33109\n",
      "[184]\ttrain-rmse:0.37592\teval-rmse:1.33132\n",
      "[185]\ttrain-rmse:0.37520\teval-rmse:1.33130\n",
      "[186]\ttrain-rmse:0.37450\teval-rmse:1.33132\n",
      "[187]\ttrain-rmse:0.37352\teval-rmse:1.33154\n",
      "[188]\ttrain-rmse:0.37253\teval-rmse:1.33168\n",
      "[189]\ttrain-rmse:0.37187\teval-rmse:1.33177\n",
      "[190]\ttrain-rmse:0.37121\teval-rmse:1.33181\n",
      "[191]\ttrain-rmse:0.37022\teval-rmse:1.33197\n",
      "[192]\ttrain-rmse:0.36934\teval-rmse:1.33219\n",
      "[193]\ttrain-rmse:0.36842\teval-rmse:1.33242\n",
      "[194]\ttrain-rmse:0.36778\teval-rmse:1.33203\n",
      "[195]\ttrain-rmse:0.36687\teval-rmse:1.33198\n",
      "[196]\ttrain-rmse:0.36598\teval-rmse:1.33220\n",
      "[197]\ttrain-rmse:0.36516\teval-rmse:1.33212\n",
      "[198]\ttrain-rmse:0.36458\teval-rmse:1.33209\n",
      "[199]\ttrain-rmse:0.36375\teval-rmse:1.33200\n",
      "[200]\ttrain-rmse:0.36288\teval-rmse:1.33217\n",
      "[201]\ttrain-rmse:0.36203\teval-rmse:1.33232\n",
      "[202]\ttrain-rmse:0.36125\teval-rmse:1.33226\n",
      "[203]\ttrain-rmse:0.36042\teval-rmse:1.33242\n",
      "[204]\ttrain-rmse:0.35988\teval-rmse:1.33248\n",
      "[205]\ttrain-rmse:0.35907\teval-rmse:1.33270\n",
      "[206]\ttrain-rmse:0.35832\teval-rmse:1.33263\n",
      "[207]\ttrain-rmse:0.35780\teval-rmse:1.33230\n",
      "[208]\ttrain-rmse:0.35701\teval-rmse:1.33243\n",
      "[209]\ttrain-rmse:0.35653\teval-rmse:1.33228\n",
      "[210]\ttrain-rmse:0.35576\teval-rmse:1.33242\n",
      "[211]\ttrain-rmse:0.35501\teval-rmse:1.33256\n",
      "[212]\ttrain-rmse:0.35424\teval-rmse:1.33269\n",
      "[213]\ttrain-rmse:0.35355\teval-rmse:1.33264\n",
      "[214]\ttrain-rmse:0.35283\teval-rmse:1.33277\n",
      "[215]\ttrain-rmse:0.35212\teval-rmse:1.33291\n",
      "[216]\ttrain-rmse:0.35165\teval-rmse:1.33263\n",
      "[217]\ttrain-rmse:0.35096\teval-rmse:1.33277\n",
      "[218]\ttrain-rmse:0.35029\teval-rmse:1.33291\n",
      "[219]\ttrain-rmse:0.34954\teval-rmse:1.33311\n",
      "[220]\ttrain-rmse:0.34887\teval-rmse:1.33332\n",
      "[221]\ttrain-rmse:0.34822\teval-rmse:1.33345\n",
      "[222]\ttrain-rmse:0.34756\teval-rmse:1.33359\n"
     ]
    }
   ],
   "source": [
    "# try some new hyper-parameters\n",
    "# smaller learning rate and smaller tree depths\n",
    "\n",
    "# tunable parameters of XGBoost\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # For regression tasks\n",
    "    'eval_metric': 'rmse',  # Use RMSE as evaluation metric\n",
    "    'eta': 0.01,  # Learning rate\n",
    "    'max_depth': 3,  # Maximum depth of trees\n",
    "    'subsample': 0.8,  # Subsampling ratio\n",
    "    'colsample_bytree': 0.8  # Column sampling ratio\n",
    "}\n",
    "\n",
    "num_round = 1000  # Max number of boosting rounds\n",
    "\n",
    "# Train the model with early stopping\n",
    "start_time = time()\n",
    "\n",
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_round,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,  # Stop after 50 rounds of no improvement\n",
    "    verbose_eval=True  # Print evaluation results during training\n",
    ")\n",
    "\n",
    "end_time = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best iteration: 172\n",
      "training time: 0.5092024803161621\n"
     ]
    }
   ],
   "source": [
    "# after training, get the best iteration\n",
    "best_iteration = bst.best_iteration\n",
    "print(f\"best iteration: {best_iteration}\")\n",
    "print(f\"training time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evalute on test set\n",
      "test_mse: 1.9158\n",
      "evaluation time: 0.0\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "print(\"evalute on test set\")\n",
    "X_test, y_test = get_data('test.csv')\n",
    "# convert to DMatrix format\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "end_time = time()\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"test_mse: {test_mse:.4f}\")\n",
    "print(f\"evaluation time: {end_time - start_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
