{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import DistilBertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained DistilBERT model\n",
    "distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Freeze all parameters of the pretrained DistilBERT model\n",
    "for param in distilbert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze pretrained DistilBert, fine tune the newly-added fc layers\n",
    "class DistilBERTWithFC(nn.Module):\n",
    "    def __init__(self, distilbert, dropout=0.1, hidden_dim=50, output_dim=1):\n",
    "        super(DistilBERTWithFC, self).__init__()\n",
    "        self.distilbert = distilbert    # Pretrained DistilBERT model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # fine tune fc layers\n",
    "        self.regressor = nn.Sequential(\n",
    "                            nn.Linear(768, hidden_dim),  # DistilBERT hidden size is 768\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(hidden_dim, output_dim),\n",
    "                        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through DistilBERT\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # The last hidden state (batch_size, seq_len, hidden_dim)\n",
    "        hidden_state = outputs.last_hidden_state\n",
    "        \n",
    "        # Use the [CLS] token representation (first token in sequence)\n",
    "        cls_token_state = hidden_state[:, 0, :]  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        cls_token_state = self.dropout(cls_token_state)\n",
    "        \n",
    "        # Pass through the custom linear layer\n",
    "        output = self.regressor(cls_token_state)  # (batch_size, output_dim)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunable parameters\n",
    "hidden_dim = 50\n",
    "dropout = 0.1\n",
    "learning_rate = 1e-3\n",
    "max_epoch = 3\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for rating prediction task\n",
    "# here we only use review_tokens as inputs, ratings as labels\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, review_tokens, labels):\n",
    "        self.review_tokens = review_tokens\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.review_tokens[idx], dtype=torch.long)\n",
    "        attention_mask = torch.ones_like(input_ids, dtype=torch.long)\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        return input_ids, attention_mask, labels\n",
    "\n",
    "\n",
    "# padding for different seq_len\n",
    "def collate_batch(batch):\n",
    "    batch_input_ids, batch_attention_mask, batch_labels = zip(*batch)\n",
    "    batch_labels = torch.stack(batch_labels)\n",
    "\n",
    "    # max seq_len in this batch\n",
    "    max_len = max([input_ids.shape[0] for input_ids in batch_input_ids])\n",
    "\n",
    "    # pad each sequence to the max seq_len\n",
    "    padded_batch_input_ids = [torch.cat((input_ids, torch.zeros(max_len - len(input_ids), dtype=torch.long))) for input_ids in batch_input_ids]\n",
    "    padded_batch_input_ids = torch.stack(padded_batch_input_ids)\n",
    "\n",
    "    padded_batch_attention_mask = torch.ones_like(padded_batch_input_ids, dtype=torch.long)\n",
    "\n",
    "    return padded_batch_input_ids, padded_batch_attention_mask, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train+valid set from csv\n",
    "def get_data(csv_file):\n",
    "    df = pd.read_csv(csv_file, sep=',')\n",
    "    all_review_tokens = []\n",
    "    all_labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        review_tokens = eval(row['review_tokens'])\n",
    "        rating = float(row['rating'])\n",
    "\n",
    "        all_review_tokens.append(review_tokens)\n",
    "        all_labels.append(rating)\n",
    "    \n",
    "    return all_review_tokens, all_labels\n",
    "\n",
    "train_review_tokens, train_labels = get_data('train.csv')\n",
    "valid_review_tokens, valid_labels = get_data('valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset and dataloader for train set\n",
    "train_dataset = RegressionDataset(train_review_tokens, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "valid_dataset = RegressionDataset(valid_review_tokens, valid_labels)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MSE on valid/test set\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for input_ids, attention_mask, labels in tqdm(dataloader):\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        outputs = outputs.squeeze()\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(outputs, labels, reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    mse = total_loss / len(dataloader.dataset)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stop if validation loss starts to increase, check every 20 iterations\n",
    "def train_step(model, train_dataloader, valid_dataloader, optimizer, loss_fn, pre_valid_mse=None):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    early_stop = False\n",
    "\n",
    "    n_iter = 0\n",
    "    valid_mse = 0.0\n",
    "\n",
    "    for input_ids, attention_mask, labels in tqdm(train_dataloader):\n",
    "        n_iter += 1\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        outputs = outputs.squeeze()\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()  # Backward pass: compute gradients\n",
    "        optimizer.step()  # Optimizer step: update weights\n",
    "\n",
    "        # evaluate on valid set to check if we need to early stop, check every 20 iterations\n",
    "        # if n_iter % 20 == 0:\n",
    "        #     valid_mse = evaluate(model, valid_dataloader)\n",
    "        #     print(f\"valid_mse: {valid_mse:.4f}\")\n",
    "        #     if (pre_valid_mse is not None and pre_valid_mse < valid_mse) or np.isnan(valid_mse):\n",
    "        #         early_stop = True\n",
    "        #         break\n",
    "        #     pre_valid_mse = valid_mse\n",
    "        \n",
    "    return early_stop, valid_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [1:04:41<00:00, 12.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [1:03:58<00:00, 12.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [1:04:24<00:00, 12.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 11583.679755926132\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# Initialize the custom model\n",
    "custom_model = DistilBERTWithFC(distilbert, dropout=dropout, hidden_dim=hidden_dim, output_dim=1)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(custom_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train\n",
    "start_time = time()\n",
    "\n",
    "pre_valid_mse = None\n",
    "for epoch in range(max_epoch):\n",
    "    print(f\"epoch {epoch + 1}\")\n",
    "\n",
    "    early_stop, valid_mse = train_step(custom_model, train_dataloader, valid_dataloader, optimizer, loss_fn, pre_valid_mse)\n",
    "\n",
    "    if early_stop:\n",
    "        print(f\"Early stop at epoch {epoch + 1}, valid_mse = {valid_mse:.4f}\")\n",
    "        break\n",
    "    pre_valid_mse = valid_mse\n",
    "\n",
    "end_time = time()\n",
    "print(f\"training time: {end_time - start_time}\")\n",
    "\n",
    "# save model\n",
    "model_path = 'distilbert_freeze_fine_tune_fc.pth'\n",
    "torch.save(custom_model, model_path)\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "evaluate on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [37:15<00:00, 11.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mse: 1.8593\n",
      "evaluation time: 2235.9321162700653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "if os.path.exists(model_path):\n",
    "    model = torch.load(model_path)\n",
    "    print(\"model loaded\")\n",
    "\n",
    "    print(\"evaluate on test set\")\n",
    "    test_review_tokens, test_labels = get_data('test.csv')\n",
    "    test_dataset = RegressionDataset(test_review_tokens, test_labels)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "    start_time = time()\n",
    "    test_mse = evaluate(model, test_dataloader)\n",
    "    end_time = time()\n",
    "    \n",
    "    print(f\"test_mse: {test_mse:.4f}\")\n",
    "    print(f\"evaluation time: {end_time - start_time}\")\n",
    "    \n",
    "else:\n",
    "    print(\"model doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "evaluate on training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [58:47<00:00, 11.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_mse: 0.8678\n",
      "evaluation time: 3527.79021692276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on train set\n",
    "model_path = 'distilbert_freeze_fine_tune_fc.pth'\n",
    "if os.path.exists(model_path):\n",
    "    model = torch.load(model_path)\n",
    "    print(\"model loaded\")\n",
    "\n",
    "    print(\"evaluate on training set\")\n",
    "\n",
    "    start_time = time()\n",
    "    training_mse = evaluate(model, train_dataloader)\n",
    "    end_time = time()\n",
    "    \n",
    "    print(f\"training_mse: {training_mse:.4f}\")\n",
    "    print(f\"evaluation time: {end_time - start_time}\")\n",
    "    \n",
    "else:\n",
    "    print(\"model doesn't exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
