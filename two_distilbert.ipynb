{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained DistilBERT model\n",
    "# here we user two distilbert models, one for recipes, one for reviews\n",
    "distilbert_review = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "distilbert_recipe = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Freeze all parameters of the pretrained DistilBERT model\n",
    "for param in distilbert_review.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in distilbert_recipe.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze pretrained DistilBert, fine tune the newly-added fc layers\n",
    "class TwoDistilBERT(nn.Module):\n",
    "    def __init__(self, distilbert_review, distilbert_recipe, dropout=0.1, hidden_dim=25, output_dim=1):\n",
    "        super(TwoDistilBERT, self).__init__()\n",
    "        # Pretrained DistilBERT model\n",
    "        self.distilbert_review = distilbert_review\n",
    "        self.distilbert_recipe = distilbert_recipe\n",
    "        self.dropout_review = nn.Dropout(dropout)\n",
    "        self.dropout_recipe = nn.Dropout(dropout)\n",
    "\n",
    "        # fine tune fc layers\n",
    "        self.regressor_review = nn.Sequential(\n",
    "                                    nn.Linear(768, hidden_dim),  # DistilBERT hidden size is 768\n",
    "                                    nn.ReLU(),\n",
    "                                )\n",
    "        \n",
    "        self.regressor_recipe = nn.Sequential(\n",
    "                                    nn.Linear(768, hidden_dim),  # DistilBERT hidden size is 768\n",
    "                                    nn.ReLU(),                                )\n",
    "\n",
    "        # combine results of review and recipe\n",
    "        self.combinator = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, review_ids, review_mask, recipe_ids, recipe_mask):\n",
    "        # for review\n",
    "        # Forward pass through DistilBERT\n",
    "        review_outputs = self.distilbert_review(input_ids=review_ids, attention_mask=review_mask)\n",
    "        \n",
    "        # The last hidden state (batch_size, seq_len, hidden_dim)\n",
    "        review_hidden_state = review_outputs.last_hidden_state\n",
    "        \n",
    "        # Use the [CLS] token representation (first token in sequence)\n",
    "        review_cls_token_state = review_hidden_state[:, 0, :]  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        review_cls_token_state = self.dropout_review(review_cls_token_state)\n",
    "        \n",
    "        # Pass through the custom linear layer\n",
    "        review_output = self.regressor_review(review_cls_token_state)  # (batch_size, hidden_dim)\n",
    "\n",
    "        # for recipe\n",
    "        # Forward pass through DistilBERT\n",
    "        recipe_outputs = self.distilbert_recipe(input_ids=recipe_ids, attention_mask=recipe_mask)\n",
    "        \n",
    "        # The last hidden state (batch_size, seq_len, hidden_dim)\n",
    "        recipe_hidden_state = recipe_outputs.last_hidden_state\n",
    "        \n",
    "        # Use the [CLS] token representation (first token in sequence)\n",
    "        recipe_cls_token_state = recipe_hidden_state[:, 0, :]  # (batch_size, hidden_dim)\n",
    "        \n",
    "        # Apply dropout for regularization\n",
    "        recipe_cls_token_state = self.dropout_recipe(recipe_cls_token_state)\n",
    "        \n",
    "        # Pass through the custom linear layer\n",
    "        recipe_output = self.regressor_recipe(recipe_cls_token_state)  # (batch_size, hidden_dim)\n",
    "\n",
    "        # combine the results of review and recipe\n",
    "        output = torch.cat((review_output, recipe_output), dim=-1)\n",
    "        output = self.combinator(output)    # (batch_size, output_dim)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tunable parameters\n",
    "hidden_dim = 25\n",
    "dropout = 0.1\n",
    "learning_rate = 1e-3\n",
    "max_epoch = 1\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for rating prediction task\n",
    "# here we use review_tokens, recipe_tokens as inputs, ratings as labels\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, review_tokens, recipe_tokens, labels):\n",
    "        self.review_tokens = review_tokens\n",
    "        self.recipe_tokens = recipe_tokens\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review_ids = torch.tensor(self.review_tokens[idx], dtype=torch.long)\n",
    "        review_mask = torch.ones_like(review_ids, dtype=torch.long)\n",
    "        recipe_ids = torch.tensor(self.recipe_tokens[idx], dtype=torch.long)\n",
    "        recipe_mask = torch.ones_like(recipe_ids, dtype=torch.long)\n",
    "        labels = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        return review_ids, review_mask, recipe_ids, recipe_mask, labels\n",
    "\n",
    "\n",
    "# padding for different seq_len\n",
    "def collate_batch(batch):\n",
    "    batch_review_ids, batch_review_mask, batch_recipe_ids, batch_recipe_mask, batch_labels = zip(*batch)\n",
    "    batch_labels = torch.stack(batch_labels)\n",
    "\n",
    "    # for review\n",
    "    # max seq_len in this batch\n",
    "    max_review_len = max([review_ids.shape[0] for review_ids in batch_review_ids])\n",
    "\n",
    "    # pad each sequence to the max seq_len\n",
    "    padded_batch_review_ids = [torch.cat((review_ids, torch.zeros(max_review_len - len(review_ids), dtype=torch.long))) for review_ids in batch_review_ids]\n",
    "    padded_batch_review_ids = torch.stack(padded_batch_review_ids)\n",
    "\n",
    "    padded_batch_review_mask = torch.ones_like(padded_batch_review_ids, dtype=torch.long)\n",
    "\n",
    "    # for recipe\n",
    "    # max seq_len in this batch\n",
    "    max_recipe_len = max([recipe_ids.shape[0] for recipe_ids in batch_recipe_ids])\n",
    "\n",
    "    # pad each sequence to the max seq_len\n",
    "    padded_batch_recipe_ids = [torch.cat((recipe_ids, torch.zeros(max_recipe_len - len(recipe_ids), dtype=torch.long))) for recipe_ids in batch_recipe_ids]\n",
    "    padded_batch_recipe_ids = torch.stack(padded_batch_recipe_ids)\n",
    "\n",
    "    padded_batch_recipe_mask = torch.ones_like(padded_batch_recipe_ids, dtype=torch.long)\n",
    "\n",
    "    return padded_batch_review_ids, padded_batch_review_mask, padded_batch_recipe_ids, padded_batch_recipe_mask, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train+valid set from csv\n",
    "def get_data(csv_file):\n",
    "    df = pd.read_csv(csv_file, sep=',')\n",
    "    all_review_tokens = []\n",
    "    all_recipe_tokens = []\n",
    "    all_labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        review_tokens = eval(row['review_tokens'])\n",
    "        recipe_tokens = eval(row['recipe_tokens'])\n",
    "        rating = float(row['rating'])\n",
    "\n",
    "        all_review_tokens.append(review_tokens)\n",
    "        all_recipe_tokens.append(recipe_tokens)\n",
    "        all_labels.append(rating)\n",
    "    \n",
    "    return all_review_tokens, all_recipe_tokens, all_labels\n",
    "\n",
    "train_review_tokens, train_recipe_tokens, train_labels = get_data('train.csv')\n",
    "valid_review_tokens, valid_recipe_tokens, valid_labels = get_data('valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset and dataloader for train set\n",
    "train_dataset = RegressionDataset(train_review_tokens, train_recipe_tokens, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "valid_dataset = RegressionDataset(valid_review_tokens, valid_recipe_tokens, valid_labels)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MSE on valid/test set\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for review_ids, review_mask, recipe_ids, recipe_mask, labels in tqdm(dataloader):\n",
    "        outputs = model(review_ids, review_mask, recipe_ids, recipe_mask)\n",
    "        outputs = outputs.squeeze()\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(outputs, labels, reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    mse = total_loss / len(dataloader.dataset)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stop if validation loss starts to increase, check every 20 iterations\n",
    "def train_step(model, train_dataloader, valid_dataloader, optimizer, loss_fn, pre_valid_mse=None):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    early_stop = False\n",
    "\n",
    "    n_iter = 0\n",
    "    valid_mse = 0.0\n",
    "\n",
    "    for review_ids, review_mask, recipe_ids, recipe_mask, labels in tqdm(train_dataloader):\n",
    "        n_iter += 1\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        outputs = model(review_ids, review_mask, recipe_ids, recipe_mask)\n",
    "        outputs = outputs.squeeze()\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()  # Backward pass: compute gradients\n",
    "        optimizer.step()  # Optimizer step: update weights\n",
    "\n",
    "        # evaluate on valid set to check if we need to early stop, check every 20 iterations\n",
    "        # if n_iter % 20 == 0:\n",
    "        #     valid_mse = evaluate(model, valid_dataloader)\n",
    "        #     print(f\"valid_mse: {valid_mse:.4f}\")\n",
    "        #     if (pre_valid_mse is not None and pre_valid_mse < valid_mse) or np.isnan(valid_mse):\n",
    "        #         early_stop = True\n",
    "        #         break\n",
    "        #     pre_valid_mse = valid_mse\n",
    "\n",
    "    return early_stop, valid_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [2:23:48<00:00, 27.57s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 8628.244009494781\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# Initialize the custom model\n",
    "custom_model = TwoDistilBERT(distilbert_review, distilbert_recipe, dropout=dropout, hidden_dim=hidden_dim, output_dim=1)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(custom_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train\n",
    "start_time = time()\n",
    "\n",
    "pre_valid_mse = None\n",
    "for epoch in range(max_epoch):\n",
    "    print(f\"epoch {epoch + 1}\")\n",
    "\n",
    "    early_stop, valid_mse = train_step(custom_model, train_dataloader, valid_dataloader, optimizer, loss_fn, pre_valid_mse)\n",
    "\n",
    "    if early_stop:\n",
    "        print(f\"Early stop at epoch {epoch + 1}, valid_mse = {valid_mse:.4f}\")\n",
    "        break\n",
    "    pre_valid_mse = valid_mse\n",
    "\n",
    "end_time = time()\n",
    "print(f\"training time: {end_time - start_time}\")\n",
    "\n",
    "# save model\n",
    "model_path = 'two_distilbert.pth'\n",
    "torch.save(custom_model, model_path)\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "evaluate on test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [1:31:46<00:00, 28.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_mse: 1.8542\n",
      "evaluation time: 5506.724800109863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "model_path = 'two_distilbert.pth'\n",
    "if os.path.exists(model_path):\n",
    "    model = torch.load(model_path)\n",
    "    print(\"model loaded\")\n",
    "\n",
    "    print(\"evaluate on test set\")\n",
    "    test_review_tokens, test_recipe_tokens, test_labels = get_data('test.csv')\n",
    "    test_dataset = RegressionDataset(test_review_tokens, test_recipe_tokens, test_labels)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "    start_time = time()\n",
    "    test_mse = evaluate(model, test_dataloader)\n",
    "    end_time = time()\n",
    "    \n",
    "    print(f\"test_mse: {test_mse:.4f}\")\n",
    "    print(f\"evaluation time: {end_time - start_time}\")\n",
    "    \n",
    "else:\n",
    "    print(\"model doesn't exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n",
      "evaluate on training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [2:09:10<00:00, 24.76s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_mse: 0.9165\n",
      "evaluation time: 7750.106264829636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate on train set\n",
    "model_path = 'two_distilbert.pth'\n",
    "if os.path.exists(model_path):\n",
    "    model = torch.load(model_path)\n",
    "    print(\"model loaded\")\n",
    "\n",
    "    print(\"evaluate on training set\")\n",
    "\n",
    "    start_time = time()\n",
    "    training_mse = evaluate(model, train_dataloader)\n",
    "    end_time = time()\n",
    "    \n",
    "    print(f\"training_mse: {training_mse:.4f}\")\n",
    "    print(f\"evaluation time: {end_time - start_time}\")\n",
    "    \n",
    "else:\n",
    "    print(\"model doesn't exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
